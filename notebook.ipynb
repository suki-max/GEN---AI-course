{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1348adc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~treamlit (C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~treamlit (C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~treamlit (C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00,  7.35 examples/s]\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_9284\\2242884826.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 02:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>61.572800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: <extra_id_0>abinsk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. Install dependencies\n",
    "# ============================================\n",
    "%pip install transformers datasets sentencepiece -q\n",
    "\n",
    "import torch\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "\n",
    "# ============================================\n",
    "# 2. Load lightweight model & tokenizer\n",
    "# ============================================\n",
    "model_name = \"google/mt5-small\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# ============================================\n",
    "# 3. Tiny toy dataset (multilingual examples)\n",
    "# ============================================\n",
    "data = {\n",
    "    \"src_text\": [\n",
    "        \"Translate English to Hindi: Hello, how are you?\",\n",
    "        \"Translate English to Spanish: I love programming.\",\n",
    "        \"Translate Hindi to English: मेरा नाम सुकृति है।\",\n",
    "        \"Translate Spanish to English: Me gusta aprender IA.\"\n",
    "    ],\n",
    "    \"tgt_text\": [\n",
    "        \"नमस्ते, आप कैसे हैं?\",\n",
    "        \"Me encanta programar.\",\n",
    "        \"My name is Sukirti.\",\n",
    "        \"I like learning AI.\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# ============================================\n",
    "# 4. Tokenization\n",
    "# ============================================\n",
    "def preprocess(batch):\n",
    "    inputs = tokenizer(batch[\"src_text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "    labels = tokenizer(batch[\"tgt_text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# ============================================\n",
    "# 5. Training setup (minimal, no errors)\n",
    "# ============================================\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5-small-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,             # just 1 epoch (fast demo)\n",
    "    logging_steps=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(), # GPU if available\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 6. Trainer\n",
    "# ============================================\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 7. Train\n",
    "# ============================================\n",
    "trainer.train()\n",
    "\n",
    "# ============================================\n",
    "# 8. Quick test\n",
    "# ============================================\n",
    "test_text = \"Translate English to Hindi: How is the weather today?\"\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=40)\n",
    "print(\"Output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
